{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ccb1d8a"
   },
   "source": [
    "# Intermediate Machine Learning: Assignment 5\n",
    "\n",
    "**Deadline**\n",
    "\n",
    "Assignment 5 is due Wednesday, December 6 by 11:59pm. Late work will not be accepted as per the course policies (see the Syllabus and Course policies on Canvas).\n",
    "\n",
    "Directly sharing answers is not okay, but discussing problems with the course staff or with other students is encouraged.\n",
    "\n",
    "You should start early so that you have time to get help if you're stuck. The drop-in office hours schedule can be found on Canvas. You can also post questions or start discussions on Ed Discussion. The assignment may look long at first glance, but the problems are broken up into steps that should help you to make steady progress.\n",
    "\n",
    "**Submission**\n",
    "\n",
    "Submit your assignment as a pdf file on Gradescope, and as a notebook (.ipynb) on Canvas. You can access Gradescope through Canvas on the left-side of the class home page. The problems in each homework assignment are numbered. Note: When submitting on Gradescope, please select the correct pages of your pdf that correspond to each problem. This will allow graders to more easily find your complete solution to each problem.\n",
    "\n",
    "To produce the .pdf, please do the following in order to preserve the cell structure of the notebook:\n",
    "\n",
    "Go to \"File\" at the top-left of your Jupyter Notebook\n",
    "Under \"Download as\", select \"HTML (.html)\"\n",
    "After the .html has downloaded, open it and then select \"File\" and \"Print\" (note you will not actually be printing)\n",
    "From the print window, select the option to save as a .pdf\n",
    "\n",
    "**Topics**\n",
    "\n",
    " * Policy iteration\n",
    " * RNNs and GRUs\n",
    "\n",
    "This assignment will also help to solidify your Python skills."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ab0544a4"
   },
   "source": [
    "## Problem 1: Reinforcement Learning: Policy Iteration (10 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "730feba7"
   },
   "source": [
    "During class, we talked about the policy iteration algorithm, which is a tabular method that estimates the optimal policy using iterative policy evaluation. \n",
    "More specifically, at each policy evaluation step, we follow a policy $\\pi$ and update the value function $V(s)$ for each state $s$ as \n",
    "\n",
    "$$\n",
    "V(s) \\gets \\sum_{s', r} p(s', r|s, \\pi(s))[r + \\gamma V(s')];\n",
    "$$\n",
    "\n",
    "at each policy improvement step, we improve the current policy $\\pi(s)$ following the updated value function: \n",
    "\n",
    "$$\n",
    "\\pi(s) \\gets \\text{argmax}_{a}\\sum_{s',r} p(s', r|s,a)[r+\\gamma V(s')].\n",
    "$$\n",
    "\n",
    "Following this iteration of policy evaluation and policy improvement, we can obtain a sequence of monotonically improving policies and value functions. In other words, each policy is guaranteed to be a strict improvement over the previous one unless it is an optimal policy, i.e., $\\pi = \\pi^*$.\n",
    "\n",
    "Since a finite MDP (Markov Decision Process) has only a finite number of policies, this process eventually converges to an optimal policy $\\pi^*$ and the corresponding optimal value function in a finite number of iterations.\n",
    "\n",
    "However, it is not entirely clear *a priori* why we are able to constantly improve our policy $\\pi$ following this alternating process. In this question, we walk you through a proof that the policy improves in each iteration of the algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbd0edfd"
   },
   "source": [
    "1. Suppose we have a policy $\\pi$ and a monotonically improved policy $\\pi'$, such that for all states $s \\in S$, we have \n",
    "\n",
    "$$\n",
    "Q_{\\pi}(s, \\pi'(s)) \\ge V_{\\pi}(s).\n",
    "$$\n",
    "\n",
    "Show that the value function $V_{\\pi'}$ dominates $V_{\\pi}$, i.e., for all states $s \\in S$\n",
    "\n",
    "$$\n",
    "V_{\\pi'}(s) \\ge V_{\\pi}(s).\n",
    "$$\n",
    "\n",
    "*hint: Consider expanding the value functions iteratively.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the following terms:\n",
    "* $f(s) = V_{\\pi'}(s) - V_{\\pi}(s)$\n",
    "* $g(s) = V_{\\pi'}(s) - Q_{\\pi}(s,\\pi'(s))$\n",
    "The condition given by the problem is equivalent to \n",
    "$$\n",
    "f(s) \\ge g(s),  \\forall s\n",
    "$$\n",
    "And my goal is to prove:\n",
    "$$\n",
    "f(s) \\ge 0 \n",
    "$$\n",
    "\n",
    "Now, let's begin the proof:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that:\n",
    "$$\n",
    "V_{\\pi'}(s) \n",
    "= r(s,\\pi'(s)) + \\gamma \\mathbb E_{s_1\\sim P (\\cdot|s,\\pi'(s))} V_{\\pi'} (s_1)  \\\\\n",
    "= r(s,\\pi'(s)) + \\gamma \\int V_{\\pi'} (s_1) p(s_1|s,\\pi'(s)) ds_1\n",
    "\\\\\n",
    "Q_{\\pi}(s,\\pi'(s)) \n",
    "= r(s,\\pi'(s)) + \\gamma \\mathbb E_{s_1\\sim P (\\cdot|s,\\pi'(s))} V_{\\pi} (s_1) \\\\\n",
    "= r(s,\\pi'(s)) + \\gamma \\int V_{\\pi} (s_1) p(s_1|s,\\pi'(s)) ds_1\n",
    "$$\n",
    "\n",
    "Now we can have:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f(s) = V_{\\pi'}(s) - V_\\pi(s) \\\\\n",
    "\\ge g(s) = V_{\\pi'} (s) - Q_\\pi(s,\\pi'(s)) \\\\\n",
    "= r(s,\\pi'(s)) +  \\gamma\\int V_{\\pi'} (s_1) p(s_1|s,\\pi'(s)) ds_1\\\\ - r(s,\\pi'(s)) +\\gamma \\int V_{\\pi} (s_1) p(s_1|s,\\pi'(s)) ds_1 \\\\\n",
    "= \\gamma\\int [V_{\\pi'} (s_1) - V_{\\pi} (s_1)] p(s_1|s,\\pi'(s)) ds_1 \\\\\n",
    "= \\gamma\\int f(s_1) p(s_1|s,\\pi'(s)) ds_1 \\\\\n",
    "= \\gamma\\mathbb E_{s_1\\sim P_1(\\cdot|s,\\pi')} f(s_1)\n",
    "$$\n",
    "where $P_t(\\cdot|s,\\pi')$ is the probablity distribution of the state $s_t$ of starting at $s$ and follow the policy $\\pi'$ for $t$ step. \n",
    "Essentially, the previous mathematical computation tells us that \n",
    "$$\n",
    "f(s) \\ge \\gamma \\mathbb E_{s_1\\sim P_1(\\cdot|s,\\pi')} f(s_1)\n",
    "$$\n",
    "\n",
    "\n",
    "By mathematical induction,\n",
    "$$\n",
    "f(s) \\ge \\gamma^t \\mathbb E_{s_t\\sim P_t(\\cdot|s,\\pi')} f(s_t),\\quad \\forall t\\in\\mathbb N\n",
    "$$\n",
    "\n",
    "Now, let's examine $s^* = \\text{argmin}_{s} f(s)$, we have that $\\forall t\\in\\mathbb N$\n",
    "$$\n",
    "f(s^*) \\ge \\gamma^t \\mathbb E_{s_t\\sim P_t(\\cdot|s^*,\\pi')} f(s_t) \\ge \\gamma^t \\mathbb E_{s_t\\sim P_t(\\cdot|s^*,\\pi')} f(s^*) = \\gamma^t f(s^*)\n",
    "$$\n",
    "\n",
    "Letting $t\\to\\infty$, the RHS of the above equation goes to $0$. Therefore\n",
    "$$\n",
    "f(s^*) \\ge 0\n",
    "$$\n",
    "And we know for a fact that $f(s)\\ge f(s^*)\\ge0$. The proof is finished. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5a7b2bda"
   },
   "source": [
    "2. Apply the conclusion of part 1 to the policy improvement step to show that it leads to a sequence of monotonically improving policies. In other words, show that if $\\pi \\neq \\pi^*$, the next round policy $\\pi'$ under the policy iteration algorithm satisfies $V_{\\pi'}(s) > V_{\\pi}(s)$ for some state $s\\in S$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use proof by contradiction: prove that given $V_{\\pi'}(s) = V_\\pi(s)$, then $\\forall s$, $\\pi'=\\pi$. \n",
    "\n",
    "Proof. \n",
    "given that $V_{\\pi'}(s) = V_\\pi(s)$ $\\forall s$, then "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "V_{\\pi}(s) = V_{\\pi'}(s) = \\sum_{s', r} p(s', r|s, \\pi'(s))[r + \\gamma V^{\\pi'}(s')] = \\sum_{s', r} p(s', r|s, \\pi'(s))[r + \\gamma V^{\\pi}(s')]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the update rule dictates that\n",
    "$$\\pi'(s) = \\text{argmax}_a \\sum_{s',r} p(s', r|s,a)[r+\\gamma V^{\\pi}(s')]$$\n",
    "we know that \n",
    "$$\n",
    "\\sum_{s', r} p(s', r|s, \\pi'(s))[r + \\gamma V^{\\pi}(s')] = \\max_a \\sum_{s', r} p(s', r|s, a)[r + \\gamma V^{\\pi}(s')]\n",
    "$$\n",
    "\n",
    "In a summary, we now have\n",
    "$$\n",
    "V_{\\pi}(s) \n",
    "= V_{\\pi'}(s) \\\\\n",
    "= \\sum_{s', r} p(s', r|s, \\pi'(s))[r + \\gamma V^{\\pi'}(s')] \\\\\n",
    "= \\sum_{s', r} p(s', r|s, \\pi'(s))[r + \\gamma V^{\\pi}(s')]  \\\\\n",
    "= \\max_a \\sum_{s', r} p(s', r|s, a)[r + \\gamma V^{\\pi}(s')] \\\\\n",
    "$$\n",
    "This is the Bellman equation that gives us the optimality of $\\pi$. \n",
    "And the proof by contradition is finished. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78f64b66"
   },
   "source": [
    "## Problem 2: Elephants Can Remember (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ba3663cd"
   },
   "source": [
    "Text generation is a common task in Natural Language Processing (NLP), where, given an initial text as a prompt, the model will produce human-like text that continues the prompt. Over the past years, transformer-based models (like GPT-3) have taken over the domain of text generation. In this problem, let's take a step back and focus on the earlier sequence models for text generation: Vanilla Recurrent Neural Networks (RNNs) and Recurrent Neural Networks with Gated Recurrent Units (GRUs). The models in this part of the assignment will be character-based models, trained on an extract of the book \"Elephants Can Remember\" by Agatha Christie. To reduce the size of our vocabulary, the text is pre-processed by converting the letters to lower case and removing numbers. The code below shows some information about our training and test set. All the necessary files for this problem are available through Canvas, under the file name \"q2_data\", a compressed folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0804721c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import random\n",
    "  \n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import GRU, SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "f93de082"
   },
   "outputs": [],
   "source": [
    "with open('q2_data/Agatha_Christie_train.txt', 'r') as file:\n",
    "    train_text = file.read()\n",
    "    \n",
    "with open('q2_data/Agatha_Christie_test.txt', 'r') as file:\n",
    "    test_text = file.read()\n",
    "\n",
    "vocabulary = sorted(list(set(train_text + test_text)))\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "# Dictionaries to go from a character to index and vice versa\n",
    "char_to_indices = dict((c, i) for i, c in enumerate(vocabulary))\n",
    "indices_to_char = dict((i, c) for i, c in enumerate(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7320ed64"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mrs. oliver looked at herself in the glass. she gave a brief, sideways look towards the clock on the mantelpiece, which she had some idea was twenty minutes slow. then she resumed her study of her coiffure. the trouble with mrs. oliver was--and she admitted it freely--that her styles of hairdressing were always being changed. she had tried almost everything in turn. a severe pompadour at one time, then a wind-swept style where you brushed back your locks to display an intellectual brow, at least'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The first 500 characters of our training set\n",
    "train_text[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3a50abac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary contains 44 characters\n",
      "The training set contains 262174 characters\n",
      "The test set contains 7209 characters\n"
     ]
    }
   ],
   "source": [
    "print(\"The vocabulary contains\", vocab_size, \"characters\")\n",
    "print(\"The training set contains\", len(train_text) ,\"characters\")\n",
    "print(\"The test set contains\", len(test_text) ,\"characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d02ce399"
   },
   "source": [
    "### Problem 2.1: The Diversity of Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6951a417"
   },
   "source": [
    "Before jumping into coding, let's start with comparing the language models we will be using in this assigment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "472741ad"
   },
   "source": [
    "1. Describe the differences between a Vanilla RNN and a GRU network. In your explanation, make sure you mention the issues with vanilla RNNs and how GRUs try to solve them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06832930"
   },
   "source": [
    "GRU has an additional gates. The first gate is the reset gates, which transforms the hidden state to be used to generate context. the 2nd gate is the memory gate, which choose from the context and the hidden state to generate the new hidden state. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61c45e86"
   },
   "source": [
    "2. Describe at least two advantages of a character based language model over a word based language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* character based model can have a simple one-hot embedding into vector space. \n",
    "* there is no computational bottleneck at the output of character based model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccac684e"
   },
   "source": [
    "### Problem 2.2: Generating Text with the Vanilla RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85bdca8b"
   },
   "source": [
    "The code below loads in a pretrained vanilla RNN model with two layers. The model is set up exactly like in the lecture slides (with tanh activation layers in the recurrent layers) with the addition of biases (intercepts) in every layer (i.e. the recurrent layer and the dense layer). The training process consisted of 30 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "45bd5d3b"
   },
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "json_file = open('q2_data/RNN_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "\n",
    "RNN_model = model_from_json(loaded_model_json)\n",
    "RNN_model.load_weights(\"q2_data/RNN_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "648273ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Vanilla_RNN_1 (SimpleRNN)   (None, 100, 128)          22144     \n",
      "                                                                 \n",
      " Vanilla_RNN_2 (SimpleRNN)   (None, 64)                12352     \n",
      "                                                                 \n",
      " Dense_layer (Dense)         (None, 44)                2860      \n",
      "                                                                 \n",
      " Softmax_layer (Activation)  (None, 44)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 37,356\n",
      "Trainable params: 37,356\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load in the weights and show summary\n",
    "weights_RNN = RNN_model.get_weights()\n",
    "RNN_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4bcccd8"
   },
   "source": [
    "Finish the following function that uses a vanilla RNN architecture to generate text, given the weights of the RNN model, a text prompt, and the number of characters to return. The function should be completed by **only using numpy functions**. Use your knowledge of how every weight plays its role in the RNN architecture. Do not worry about the weight extraction part, this is already provided for you. The weight matrix $W_{xh1}$, for example, denotes the weight matrix to go from the input x to the first hidden state layer h1. The hidden states $h_1$ and $h_2$ are initialized to a vector of zeros. \n",
    "\n",
    "The embedding of each character has to be done by a one-hot encoding, where you will need the dictionaries defined in the introduction to go from a character to an index position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44, 128)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_RNN[0].shape, weights_RNN[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "736ff633"
   },
   "outputs": [],
   "source": [
    "def sample_text_RNN(weights, prompt, N):\n",
    "    '''\n",
    "    Uses a pretrained RNN to generate text, starting from a prompt, \n",
    "    only using the weights and numpy commands\n",
    "            Parameters:\n",
    "                    weights (list): Weights of the pretrained RNN model\n",
    "                    prompt (string): Start of generated sentence\n",
    "                    N (int): Length of output sentence (including prompt)\n",
    "            Returns:\n",
    "                    output_sentence (string): Text generated by RNN\n",
    "    '''\n",
    "    # Extracting weights and biases\n",
    "    # Dimensions of matrices are same format as lecture slides\n",
    "\n",
    "    # First Recurrent Layer \n",
    "    W_xh1 = weights[0].T \n",
    "    W_h1h1 = weights[1].T \n",
    "    b_h1 = np.expand_dims(weights[2], axis=1)\n",
    "\n",
    "    # Second Recurrent Layer\n",
    "    W_h1h2 = weights[3].T\n",
    "    W_h2h2 = weights[4].T\n",
    "    b_h2 = np.expand_dims(weights[5], axis=1)\n",
    "\n",
    "    # Linear (dense) layer\n",
    "    W_h2y = weights[6].T\n",
    "    b_y = np.expand_dims(weights[7], axis=1)\n",
    "    \n",
    "    # Initiate the hidden states\n",
    "    h1 = np.zeros((W_h1h1.shape[0], 1))\n",
    "    h2 = np.zeros((W_h2h2.shape[0], 1))\n",
    "    \n",
    "    # -----------------------------------------------\n",
    "    \n",
    "    # Your code starts here\n",
    "    \n",
    "    output_sentence = ''\n",
    "    for i in range(N):\n",
    "        \n",
    "        if i < len(prompt):\n",
    "            output_sentence += prompt[i]\n",
    "        else:\n",
    "            output_sentence += y\n",
    "        \n",
    "        # onehot embedding\n",
    "        x = output_sentence[i]\n",
    "        x = char_to_indices[x]\n",
    "        x_one_hot = np.zeros((vocab_size, 1))\n",
    "        x_one_hot[x] = 1\n",
    "        x = x_one_hot\n",
    "        \n",
    "        # apply RNN\n",
    "        h1 = np.tanh(np.dot(W_h1h1,h1) + np.dot(W_xh1,x) + b_h1)\n",
    "        h2 = np.tanh(np.dot(W_h2h2,h2) + np.dot(W_h1h2,h1) + b_h2)\n",
    "        y = np.dot(W_h2y,h2) + b_y\n",
    "        y = np.exp(y) / np.sum(np.exp(y))\n",
    "        \n",
    "        # sample from the distribution p\n",
    "        y = np.random.choice(range(vocab_size), p=y.ravel())\n",
    "        \n",
    "        # transform to character\n",
    "        y = indices_to_char[y]\n",
    "        \n",
    "    return output_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfc7420f"
   },
   "source": [
    "Test out your function by running the following code cell. Use it as a sanity check that your code is working. The generated text should not be perfect English, but at least you should be able to recognize some words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "12458ad0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mrs. oliver looked at herself in the glass. she gave a brief, sideways looked are somerough or your soppenilate, but they heare apting celing on he tagh there was at as elephants lyodd the, you wanted aok there it for apseancile because call coustil two menty. not,\" said mrs. oliver. \"i'm ongen all thas moseird things he have rement?\" \"no. are mracwabach, and so have yes. corden you beart, argut abdear from once things that sent abluthord, are live something people? you lought whow you doel no that there. i'm not no,\" said daypressing to to cally, rading commint ot havenscrofn. then they'd. i ence faccuthere,\" said mrvatyer. you knem shad,\" said mrs. oliver. anywas, they was her. i meane?\" \"and it's it was afted to the wistsuse. had beel lings. \"i have bely wigh rell really that have been ttrentle infieved i've been ceable things and crust of happen, helpersully wey-,\" said mrs. oliver, it's someable shos, i say, it bearny suanes forgetly you lust came well you werentiestly ele,\" said \n"
     ]
    }
   ],
   "source": [
    "print(sample_text_RNN(weights_RNN, \n",
    "                      'mrs. oliver looked at herself in the glass. she gave a brief, sideways look', \n",
    "                      1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c950eae8"
   },
   "source": [
    "### Problem 2.3: Generating Text with the GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1dd93ee7"
   },
   "source": [
    "The code below loads in a pretrained GRU model. The model is set up exactly like in the lecture slides (with sigmoid activation layers for the gates and tanh activation layers in the recurrent layer). The model is trained for only 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "82291909"
   },
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "json_file = open('q2_data/GRU_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "\n",
    "GRU_model = model_from_json(loaded_model_json)\n",
    "GRU_model.load_weights(\"q2_data/GRU_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "738974b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru (GRU)                   (None, 512)               857088    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 44)                22572     \n",
      "                                                                 \n",
      " activation (Activation)     (None, 44)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 879,660\n",
      "Trainable params: 879,660\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load in the weights and show summary\n",
    "weights_GRU = GRU_model.get_weights()\n",
    "GRU_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cad5b34f"
   },
   "source": [
    "Finish the following function that uses a GRU architecture to generate text, given the weights of the GRU model, a text prompt, and the number of characters to return. The function should be completed by **only using numpy functions**. Use your knowledge of how every weight plays its role in the GRU architecture. Do not worry about the weight extraction part, this is already provided for you. The hidden state $h$ is initialized to a vector of zeros. \n",
    "\n",
    "The embedding of each character has to be done by a one-hot encoding, where you will need the dictionaries defined in the introduction to go from a character to an index position.\n",
    "\n",
    "Note: a slightly different version of the GRU is used, where the candidate state $c_t$ is calculated as:\n",
    "\n",
    "$$\n",
    "c_t = \\text{tanh} \\left(W_{hx} x_t \\ + \\ \\Gamma_t^r \\odot (W_{hh} h_{t-1}) \\ + \\ b_h \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "259c1d40"
   },
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sample_text_GRU(weights, prompt, N):\n",
    "    '''\n",
    "    Uses a pretrained GRU to generate text, starting from a prompt,\n",
    "    only using the weights and numpy commands\n",
    "            Parameters:\n",
    "                    weights (list): Weights of the pretrained GRU model\n",
    "                    prompt (string): Start of generated sentence\n",
    "                    N (int): Total length of output sentence\n",
    "            Returns:\n",
    "                    output_sentence (string): Text generated by GRU\n",
    "    '''\n",
    "    # Extracting weights and biases\n",
    "    # Dimensions of matrices are same format as lecture slides\n",
    "    \n",
    "    # GRU Layer \n",
    "    W_ux, W_rx, W_hx = np.split(weights[0].T, 3, axis = 0)\n",
    "    W_uh, W_rh, W_hh = np.split(weights[1].T, 3, axis = 0)\n",
    "\n",
    "    bias = np.sum(weights[2], axis=0)\n",
    "    b_u, b_r, b_h = np.split(np.expand_dims(bias, axis=1), 3)\n",
    "\n",
    "    # Linear (dense) layer\n",
    "    W_y = weights[3].T\n",
    "    b_y = np.expand_dims(weights[4], axis=1)\n",
    "    \n",
    "    # Initiate hidden state\n",
    "    h = np.zeros((W_hh.shape[0], 1))\n",
    "    \n",
    "    # -----------------------------------------------\n",
    "    \n",
    "    # Your code starts here\n",
    "    output_sentence = \"\"\n",
    "    for i in range(N):\n",
    "        if i < len(prompt):\n",
    "            output_sentence += prompt[i]\n",
    "        else:\n",
    "            output_sentence += y\n",
    "        \n",
    "        # onehot embedding\n",
    "        x = output_sentence[i]\n",
    "        x = char_to_indices[x]\n",
    "        x_one_hot = np.zeros((vocab_size, 1))\n",
    "        x_one_hot[x] = 1\n",
    "        x = x_one_hot\n",
    "        \n",
    "        # apply GRU\n",
    "        u = sigmoid(np.dot(W_ux,x) + np.dot(W_uh,h) + b_u)\n",
    "        r = sigmoid(np.dot(W_rx,x) + np.dot(W_rh,h) + b_r)\n",
    "        c = np.tanh(np.dot(W_hx,x) + np.dot(r*W_hh,h) + b_h)\n",
    "        h = (1-u)*c + u*h\n",
    "        \n",
    "        y = np.dot(W_y,h) + b_y\n",
    "        y = np.exp(y) / np.sum(np.exp(y))\n",
    "        y = np.random.choice(range(vocab_size), p=y.ravel())\n",
    "        y = indices_to_char[y]\n",
    "        \n",
    "    return output_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "117fdc8f"
   },
   "source": [
    "Test out your function by running the following code cell. Use it as a sanity check that your code is working. The generated text should not be perfect English, but at least you should be able to recognize some words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "3fc14584"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mrs. oliver looked at herself in the glass. she gave a brief, sideways looked erserout grose, thatiscaveryon with meround was it it. it was to take to want to do ising these time, a terem me. entwansien your very nectly retten know at and to tell of a mind aroustite and tase, to tell you see she interestenting. it or any lifing to allore, appare.\" \"that is the itat of sprofies. to destous to feen a little to itive a it to it. not who wrething it it was a tomestly i mother hain at, and ralensione lofionicg the tage. ot. i moy sot wo lovely at thes it eleehant came she worave it it. caresting him at a lot uf the honaull about the mother of ever the two looked intelesten. i there, treit or live there any more enalooay a prestiof t lisientry into the place. me twly whe knew the atel her some one came to marry. perhaps of edottoction as i tookicatedisame hell or something remember about or the trove of a trout hir lifes by a littre sibte ard the celia looked at alw yed seem dirot and how di\n"
     ]
    }
   ],
   "source": [
    "print(sample_text_GRU(weights_GRU, \n",
    "                      'mrs. oliver looked at herself in the glass. she gave a brief, sideways look',\n",
    "                      1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6a35ba68"
   },
   "source": [
    "### Problem 2.4: Can Elephants Remember Better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "09cd85c4"
   },
   "source": [
    "Perplexity is a measure to quantify how \"good\" a language model $M$ is, based on a test (or validation) set. The perplexity on a sequence $s$ of characters $a_i$ of size $N$ is defined as:\n",
    "\n",
    "$$\n",
    "\\text{Perplexity}(M) = M(s)^{(-1/N)} = \\left\\{p(a_1, \\ldots, a_N)\\right\\}^{(-1/N)} = \\left\\{p(a_1) \\ p(a_2|a_1) \\ \\ldots \\ p(a_N|a_1, \\ldots, a_{N-1})\\right\\}^{(-1/N)}\n",
    "$$\n",
    "\n",
    "> The intuition behind this metric is that, if a model assigns a high probability to a test set, it is not surprised to see it (not perplexed by it), which means the model $M$ has a good understanding of how the language works. Hence, a good model has, in theory, a lower perplexity. The exponent $(-1/N)$ in the formula is just a normalizing strategy (geometric average), because adding more characters to a test set would otherwise introduce more uncertainty (i.e. larger test sets would have lower probability). So by introducing the geometric average, we have a metric that is independent of the size of the test set.\n",
    "\n",
    "When calculating the perplexity, it is important to know that taking the product of a bunch of probabilities will most likely lead to a zero value by the computer. To prevent this, make use of a log-transformation:\n",
    "\n",
    "$$\n",
    "\\text{Log-Perplexity}(M) = -\\frac{1}{N} log\\left\\{p(a_1, \\ldots, a_N)\\right\\} = -\\frac{1}{N} \\left\\{log \\ p(a_1) + \\ log \\ p(a_2|a_1) + \\ \\ldots \\ + log \\  p(a_N|a_1, \\ldots, a_{N-1})\\right\\} \n",
    "$$\n",
    "\n",
    "Don't forget to go back to the normal perplexity after this transformation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "709c6d4e"
   },
   "source": [
    "1. Before calculating the perplexity of a test sequence, start with comparing the outputs of 2.2 and 2.3. Do you see any differences in the generated text of the Vanilla RNN model and the GRU model? Rerun your functions a couple of times (because of stochasticity) and use different prompts. Briefly discuss why you would expect (or not expect) certain differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ccf8083"
   },
   "source": [
    "Both models start to become worse when generating far away from the prompt. However, the GRU model seems to be able to remember more of the prompt and/or the training data, such that in all the non-sense characters it generated, there will be a few times that it actually generate some proper English sentences. \n",
    "\n",
    "Also, the generated text of RNN become worse sooner than the GRU. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5c9a5bf5"
   },
   "source": [
    "2. Calculate the perplexity of each language model by using test_text, an unseen extract of the book. Choose the prompt as the first $m$ letters of the test set, where $m$ is a parameter that you can choose yourself. You should be able to reuse the majority of your previous code in this calculation. Discuss your results at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity_rnn(weights,text,m):\n",
    "    # Extracting weights and biases\n",
    "    # Dimensions of matrices are same format as lecture slides\n",
    "\n",
    "    # First Recurrent Layer \n",
    "    W_xh1 = weights[0].T \n",
    "    W_h1h1 = weights[1].T \n",
    "    b_h1 = np.expand_dims(weights[2], axis=1)\n",
    "\n",
    "    # Second Recurrent Layer\n",
    "    W_h1h2 = weights[3].T\n",
    "    W_h2h2 = weights[4].T\n",
    "    b_h2 = np.expand_dims(weights[5], axis=1)\n",
    "\n",
    "    # Linear (dense) layer\n",
    "    W_h2y = weights[6].T\n",
    "    b_y = np.expand_dims(weights[7], axis=1)\n",
    "    \n",
    "    # Initiate the hidden states\n",
    "    h1 = np.zeros((W_h1h1.shape[0], 1))\n",
    "    h2 = np.zeros((W_h2h2.shape[0], 1))\n",
    "    \n",
    "    # -----------------------------------------------\n",
    "    \n",
    "    # Your code starts here\n",
    "    log_p = 0\n",
    "\n",
    "    for i in range(len(text)-1):\n",
    "        # onehot embedding\n",
    "        x = text[i]\n",
    "        x = char_to_indices[x]\n",
    "        x_one_hot = np.zeros((vocab_size, 1))\n",
    "        x_one_hot[x] = 1\n",
    "        x = x_one_hot\n",
    "        \n",
    "        # apply RNN\n",
    "        h1 = np.tanh(np.dot(W_h1h1,h1) + np.dot(W_xh1,x) + b_h1)\n",
    "        h2 = np.tanh(np.dot(W_h2h2,h2) + np.dot(W_h1h2,h1) + b_h2)\n",
    "        \n",
    "        if i < m-1:\n",
    "            # still prompting, no need to calculate log_p\n",
    "            continue\n",
    "        \n",
    "        # get the probability of the next character\n",
    "        y = np.dot(W_h2y,h2) + b_y\n",
    "        y = np.exp(y) / np.sum(np.exp(y))\n",
    "        next_char = text[i+1]\n",
    "        next_char = char_to_indices[next_char]\n",
    "        log_p += np.log(y[next_char])\n",
    "    \n",
    "    return np.exp(-log_p/(len(text)-m))\n",
    "\n",
    "def perplexity_gru(weights,text,m):\n",
    "    # Extracting weights and biases\n",
    "    # Dimensions of matrices are same format as lecture slides\n",
    "    \n",
    "    # GRU Layer \n",
    "    W_ux, W_rx, W_hx = np.split(weights[0].T, 3, axis = 0)\n",
    "    W_uh, W_rh, W_hh = np.split(weights[1].T, 3, axis = 0)\n",
    "\n",
    "    bias = np.sum(weights[2], axis=0)\n",
    "    b_u, b_r, b_h = np.split(np.expand_dims(bias, axis=1), 3)\n",
    "\n",
    "    # Linear (dense) layer\n",
    "    W_y = weights[3].T\n",
    "    b_y = np.expand_dims(weights[4], axis=1)\n",
    "    \n",
    "    # Initiate hidden state\n",
    "    h = np.zeros((W_hh.shape[0], 1))\n",
    "    \n",
    "    # -----------------------------------------------\n",
    "    \n",
    "    # Your code starts here\n",
    "    log_p = 0\n",
    "    \n",
    "    for i in range(len(text)-1):        \n",
    "        # onehot embedding\n",
    "        x = text[i]\n",
    "        x = char_to_indices[x]\n",
    "        x_one_hot = np.zeros((vocab_size, 1))\n",
    "        x_one_hot[x] = 1\n",
    "        x = x_one_hot\n",
    "        \n",
    "        # apply GRU\n",
    "        u = sigmoid(np.dot(W_ux,x) + np.dot(W_uh,h) + b_u)\n",
    "        r = sigmoid(np.dot(W_rx,x) + np.dot(W_rh,h) + b_r)\n",
    "        c = np.tanh(np.dot(W_hx,x) + np.dot(r*W_hh,h) + b_h)\n",
    "        h = (1-u)*c + u*h\n",
    "        \n",
    "        if i < m-1:\n",
    "            # still prompting, no need to calculate log_p\n",
    "            continue\n",
    "        \n",
    "        y = np.dot(W_y,h) + b_y\n",
    "        y = np.exp(y) / np.sum(np.exp(y))\n",
    "        \n",
    "        next_char = text[i+1]\n",
    "        next_char = char_to_indices[next_char]\n",
    "        log_p += np.log(y[next_char])\n",
    "        \n",
    "    return np.exp(-log_p/(len(text)-m))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity with m = 1 is RNN = 5.6653, GRU = 3.9346\n",
      "Perplexity with m = 5 is RNN = 5.665, GRU = 3.9341\n",
      "Perplexity with m = 10 is RNN = 5.6664, GRU = 3.9357\n",
      "Perplexity with m = 20 is RNN = 5.6668, GRU = 3.9364\n",
      "Perplexity with m = 50 is RNN = 5.6572, GRU = 3.9361\n",
      "Perplexity with m = 100 is RNN = 5.666, GRU = 3.9406\n"
     ]
    }
   ],
   "source": [
    "for m in [1, 5, 10, 20, 50, 100]:    \n",
    "    print(\"Perplexity with m = {} is RNN = {}, GRU = {}\".format(\n",
    "        m, \n",
    "        perplexity_rnn(weights_RNN, test_text, m)[0].round(4),\n",
    "        perplexity_gru(weights_GRU, test_text, m)[0].round(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity with m = 2000 is RNN = 5.6714, GRU = 3.9435\n"
     ]
    }
   ],
   "source": [
    "m = 2000\n",
    "print(\"Perplexity with m = {} is RNN = {}, GRU = {}\".format(\n",
    "        m, \n",
    "        perplexity_rnn(weights_RNN, test_text, m)[0].round(4),\n",
    "        perplexity_gru(weights_GRU, test_text, m)[0].round(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations\n",
    "* GRU performs much better than RNN, which is expected\n",
    "* the value of $m$ does not matter too much for both models. This is also expected as well, all the hidden variables is the same regardless of the value of $m$, so the effect of $m$ is minimal on the value of perplexity if $m<<$ size of test_text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ab728c9"
   },
   "source": [
    "3. As seen in part 2 and 3 of this problem, the text generation is not perfect. Describe some possible model improvements that could make the quality of the generated text better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* a larger hidden state dimension\n",
    "* more layers with residual connection for the hidden states\n",
    "* apply attention mechanism\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
