{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ccb1d8a"
   },
   "source": [
    "# Intermediate Machine Learning: Assignment 5\n",
    "\n",
    "**Deadline**\n",
    "\n",
    "Assignment 5 is due Wednesday, December 6 by 11:59pm. Late work will not be accepted as per the course policies (see the Syllabus and Course policies on Canvas).\n",
    "\n",
    "Directly sharing answers is not okay, but discussing problems with the course staff or with other students is encouraged.\n",
    "\n",
    "You should start early so that you have time to get help if you're stuck. The drop-in office hours schedule can be found on Canvas. You can also post questions or start discussions on Ed Discussion. The assignment may look long at first glance, but the problems are broken up into steps that should help you to make steady progress.\n",
    "\n",
    "**Submission**\n",
    "\n",
    "Submit your assignment as a pdf file on Gradescope, and as a notebook (.ipynb) on Canvas. You can access Gradescope through Canvas on the left-side of the class home page. The problems in each homework assignment are numbered. Note: When submitting on Gradescope, please select the correct pages of your pdf that correspond to each problem. This will allow graders to more easily find your complete solution to each problem.\n",
    "\n",
    "To produce the .pdf, please do the following in order to preserve the cell structure of the notebook:\n",
    "\n",
    "Go to \"File\" at the top-left of your Jupyter Notebook\n",
    "Under \"Download as\", select \"HTML (.html)\"\n",
    "After the .html has downloaded, open it and then select \"File\" and \"Print\" (note you will not actually be printing)\n",
    "From the print window, select the option to save as a .pdf\n",
    "\n",
    "**Topics**\n",
    "\n",
    " * Policy iteration\n",
    " * RNNs and GRUs\n",
    "\n",
    "This assignment will also help to solidify your Python skills."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ab0544a4"
   },
   "source": [
    "## Problem 1: Reinforcement Learning: Policy Iteration (10 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "730feba7"
   },
   "source": [
    "During class, we talked about the policy iteration algorithm, which is a tabular method that estimates the optimal policy using iterative policy evaluation. \n",
    "More specifically, at each policy evaluation step, we follow a policy $\\pi$ and update the value function $V(s)$ for each state $s$ as \n",
    "\n",
    "$$\n",
    "V(s) \\gets \\sum_{s', r} p(s', r|s, \\pi(s))[r + \\gamma V(s')];\n",
    "$$\n",
    "\n",
    "at each policy improvement step, we improve the current policy $\\pi(s)$ following the updated value function: \n",
    "\n",
    "$$\n",
    "\\pi(s) \\gets \\text{argmax}_{a}\\sum_{s',r} p(s', r|s,a)[r+\\gamma V(s')].\n",
    "$$\n",
    "\n",
    "Following this iteration of policy evaluation and policy improvement, we can obtain a sequence of monotonically improving policies and value functions. In other words, each policy is guaranteed to be a strict improvement over the previous one unless it is an optimal policy, i.e., $\\pi = \\pi^*$.\n",
    "\n",
    "Since a finite MDP (Markov Decision Process) has only a finite number of policies, this process eventually converges to an optimal policy $\\pi^*$ and the corresponding optimal value function in a finite number of iterations.\n",
    "\n",
    "However, it is not entirely clear *a priori* why we are able to constantly improve our policy $\\pi$ following this alternating process. In this question, we walk you through a proof that the policy improves in each iteration of the algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbd0edfd"
   },
   "source": [
    "1. Suppose we have a policy $\\pi$ and a monotonically improved policy $\\pi'$, such that for all states $s \\in S$, we have \n",
    "\n",
    "$$\n",
    "Q_{\\pi}(s, \\pi'(s)) \\ge V_{\\pi}(s).\n",
    "$$\n",
    "\n",
    "Show that the value function $V_{\\pi'}$ dominates $V_{\\pi}$, i.e., for all states $s \\in S$\n",
    "\n",
    "$$\n",
    "V_{\\pi'}(s) \\ge V_{\\pi}(s).\n",
    "$$\n",
    "\n",
    "*hint: Consider expanding the value functions iteratively.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e62d7707"
   },
   "outputs": [],
   "source": [
    "# your markdown here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5a7b2bda"
   },
   "source": [
    "2. Apply the conclusion of part 1 to the policy improvement step to show that it leads to a sequence of monotonically improving policies. In other words, show that if $\\pi \\neq \\pi^*$, the next round policy $\\pi'$ under the policy iteration algorithm satisfies $V_{\\pi'}(s) > V_{\\pi}(s)$ for some state $s\\in S$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eede565f"
   },
   "outputs": [],
   "source": [
    "# your markdown here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78f64b66"
   },
   "source": [
    "## Problem 2: Elephants Can Remember (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ba3663cd"
   },
   "source": [
    "Text generation is a common task in Natural Language Processing (NLP), where, given an initial text as a prompt, the model will produce human-like text that continues the prompt. Over the past years, transformer-based models (like GPT-3) have taken over the domain of text generation. In this problem, let's take a step back and focus on the earlier sequence models for text generation: Vanilla Recurrent Neural Networks (RNNs) and Recurrent Neural Networks with Gated Recurrent Units (GRUs). The models in this part of the assignment will be character-based models, trained on an extract of the book \"Elephants Can Remember\" by Agatha Christie. To reduce the size of our vocabulary, the text is pre-processed by converting the letters to lower case and removing numbers. The code below shows some information about our training and test set. All the necessary files for this problem are available through Canvas, under the file name \"q2_data\", a compressed folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0804721c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import random\n",
    "  \n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import GRU, SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "f93de082"
   },
   "outputs": [],
   "source": [
    "with open('q2_data/Agatha_Christie_train.txt', 'r') as file:\n",
    "    train_text = file.read()\n",
    "    \n",
    "with open('q2_data/Agatha_Christie_test.txt', 'r') as file:\n",
    "    test_text = file.read()\n",
    "\n",
    "vocabulary = sorted(list(set(train_text + test_text)))\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "# Dictionaries to go from a character to index and vice versa\n",
    "char_to_indices = dict((c, i) for i, c in enumerate(vocabulary))\n",
    "indices_to_char = dict((i, c) for i, c in enumerate(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7320ed64"
   },
   "outputs": [],
   "source": [
    "# The first 500 characters of our training set\n",
    "train_text[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3a50abac"
   },
   "outputs": [],
   "source": [
    "print(\"The vocabulary contains\", vocab_size, \"characters\")\n",
    "print(\"The training set contains\", len(train_text) ,\"characters\")\n",
    "print(\"The test set contains\", len(test_text) ,\"characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d02ce399"
   },
   "source": [
    "### Problem 2.1: The Diversity of Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6951a417"
   },
   "source": [
    "Before jumping into coding, let's start with comparing the language models we will be using in this assigment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "472741ad"
   },
   "source": [
    "1. Describe the differences between a Vanilla RNN and a GRU network. In your explanation, make sure you mention the issues with vanilla RNNs and how GRUs try to solve them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "06832930"
   },
   "outputs": [],
   "source": [
    "# Your markdown here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61c45e86"
   },
   "source": [
    "2. Describe at least two advantages of a character based language model over a word based language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8b07db8a"
   },
   "outputs": [],
   "source": [
    "# Your markdown here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccac684e"
   },
   "source": [
    "### Problem 2.2: Generating Text with the Vanilla RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85bdca8b"
   },
   "source": [
    "The code below loads in a pretrained vanilla RNN model with two layers. The model is set up exactly like in the lecture slides (with tanh activation layers in the recurrent layers) with the addition of biases (intercepts) in every layer (i.e. the recurrent layer and the dense layer). The training process consisted of 30 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "45bd5d3b"
   },
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "json_file = open('q2_data/RNN_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "\n",
    "RNN_model = model_from_json(loaded_model_json)\n",
    "RNN_model.load_weights(\"q2_data/RNN_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "648273ea"
   },
   "outputs": [],
   "source": [
    "# load in the weights and show summary\n",
    "weights_RNN = RNN_model.get_weights()\n",
    "RNN_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4bcccd8"
   },
   "source": [
    "Finish the following function that uses a vanilla RNN architecture to generate text, given the weights of the RNN model, a text prompt, and the number of characters to return. The function should be completed by **only using numpy functions**. Use your knowledge of how every weight plays its role in the RNN architecture. Do not worry about the weight extraction part, this is already provided for you. The weight matrix $W_{xh1}$, for example, denotes the weight matrix to go from the input x to the first hidden state layer h1. The hidden states $h_1$ and $h_2$ are initialized to a vector of zeros. \n",
    "\n",
    "The embedding of each character has to be done by a one-hot encoding, where you will need the dictionaries defined in the introduction to go from a character to an index position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "736ff633"
   },
   "outputs": [],
   "source": [
    "def sample_text_RNN(weights, prompt, N):\n",
    "    '''\n",
    "    Uses a pretrained RNN to generate text, starting from a prompt, \n",
    "    only using the weights and numpy commands\n",
    "            Parameters:\n",
    "                    weights (list): Weights of the pretrained RNN model\n",
    "                    prompt (string): Start of generated sentence\n",
    "                    N (int): Length of output sentence (including prompt)\n",
    "            Returns:\n",
    "                    output_sentence (string): Text generated by RNN\n",
    "    '''\n",
    "    # Extracting weights and biases\n",
    "    # Dimensions of matrices are same format as lecture slides\n",
    "\n",
    "    # First Recurrent Layer \n",
    "    W_xh1 = weights[0].T \n",
    "    W_h1h1 = weights[1].T \n",
    "    b_h1 = np.expand_dims(weights[2], axis=1)\n",
    "\n",
    "    # Second Recurrent Layer\n",
    "    W_h1h2 = weights[3].T\n",
    "    W_h2h2 = weights[4].T\n",
    "    b_h2 = np.expand_dims(weights[5], axis=1)\n",
    "\n",
    "    # Linear (dense) layer\n",
    "    W_h2y = weights[6].T\n",
    "    b_y = np.expand_dims(weights[7], axis=1)\n",
    "    \n",
    "    # Initiate the hidden states\n",
    "    h1 = np.zeros((W_h1h1.shape[0], 1))\n",
    "    h2 = np.zeros((W_h2h2.shape[0], 1))\n",
    "    \n",
    "    # -----------------------------------------------\n",
    "    \n",
    "    # Your code starts here\n",
    "    output_sentence = \"\"\n",
    "        \n",
    "    return output_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfc7420f"
   },
   "source": [
    "Test out your function by running the following code cell. Use it as a sanity check that your code is working. The generated text should not be perfect English, but at least you should be able to recognize some words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "12458ad0"
   },
   "outputs": [],
   "source": [
    "print(sample_text_RNN(weights_RNN, \n",
    "                      'mrs. oliver looked at herself in the glass. she gave a brief, sideways look', \n",
    "                      1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c950eae8"
   },
   "source": [
    "### Problem 2.3: Generating Text with the GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1dd93ee7"
   },
   "source": [
    "The code below loads in a pretrained GRU model. The model is set up exactly like in the lecture slides (with sigmoid activation layers for the gates and tanh activation layers in the recurrent layer). The model is trained for only 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "82291909"
   },
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "json_file = open('q2_data/GRU_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "\n",
    "GRU_model = model_from_json(loaded_model_json)\n",
    "GRU_model.load_weights(\"q2_data/GRU_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "738974b5"
   },
   "outputs": [],
   "source": [
    "# load in the weights and show summary\n",
    "weights_GRU = GRU_model.get_weights()\n",
    "GRU_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cad5b34f"
   },
   "source": [
    "Finish the following function that uses a GRU architecture to generate text, given the weights of the GRU model, a text prompt, and the number of characters to return. The function should be completed by **only using numpy functions**. Use your knowledge of how every weight plays its role in the GRU architecture. Do not worry about the weight extraction part, this is already provided for you. The hidden state $h$ is initialized to a vector of zeros. \n",
    "\n",
    "The embedding of each character has to be done by a one-hot encoding, where you will need the dictionaries defined in the introduction to go from a character to an index position.\n",
    "\n",
    "Note: a slightly different version of the GRU is used, where the candidate state $c_t$ is calculated as:\n",
    "\n",
    "$$\n",
    "c_t = \\text{tanh} \\left(W_{hx} x_t \\ + \\ \\Gamma_t^r \\odot (W_{hh} h_{t-1}) \\ + \\ b_h \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "259c1d40"
   },
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sample_text_GRU(weights, prompt, N):\n",
    "    '''\n",
    "    Uses a pretrained GRU to generate text, starting from a prompt,\n",
    "    only using the weights and numpy commands\n",
    "            Parameters:\n",
    "                    weights (list): Weights of the pretrained GRU model\n",
    "                    prompt (string): Start of generated sentence\n",
    "                    N (int): Total length of output sentence\n",
    "            Returns:\n",
    "                    output_sentence (string): Text generated by GRU\n",
    "    '''\n",
    "    # Extracting weights and biases\n",
    "    # Dimensions of matrices are same format as lecture slides\n",
    "    \n",
    "    # GRU Layer \n",
    "    W_ux, W_rx, W_hx = np.split(weights[0].T, 3, axis = 0)\n",
    "    W_uh, W_rh, W_hh = np.split(weights[1].T, 3, axis = 0)\n",
    "\n",
    "    bias = np.sum(weights[2], axis=0)\n",
    "    b_u, b_r, b_h = np.split(np.expand_dims(bias, axis=1), 3)\n",
    "\n",
    "    # Linear (dense) layer\n",
    "    W_y = weights[3].T\n",
    "    b_y = np.expand_dims(weights[4], axis=1)\n",
    "    \n",
    "    # Initiate hidden state\n",
    "    h = np.zeros((W_hh.shape[0], 1))\n",
    "    \n",
    "    # -----------------------------------------------\n",
    "    \n",
    "    # Your code starts here\n",
    "    output_sentence = \"\"\n",
    "        \n",
    "    return output_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "117fdc8f"
   },
   "source": [
    "Test out your function by running the following code cell. Use it as a sanity check that your code is working. The generated text should not be perfect English, but at least you should be able to recognize some words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3fc14584"
   },
   "outputs": [],
   "source": [
    "print(sample_text_GRU(weights_GRU, \n",
    "                      'mrs. oliver looked at herself in the glass. she gave a brief, sideways look',\n",
    "                      1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6a35ba68"
   },
   "source": [
    "### Problem 2.4: Can Elephants Remember Better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "09cd85c4"
   },
   "source": [
    "Perplexity is a measure to quantify how \"good\" a language model $M$ is, based on a test (or validation) set. The perplexity on a sequence $s$ of characters $a_i$ of size $N$ is defined as:\n",
    "\n",
    "$$\n",
    "\\text{Perplexity}(M) = M(s)^{(-1/N)} = \\left\\{p(a_1, \\ldots, a_N)\\right\\}^{(-1/N)} = \\left\\{p(a_1) \\ p(a_2|a_1) \\ \\ldots \\ p(a_N|a_1, \\ldots, a_{N-1})\\right\\}^{(-1/N)}\n",
    "$$\n",
    "\n",
    "> The intuition behind this metric is that, if a model assigns a high probability to a test set, it is not surprised to see it (not perplexed by it), which means the model $M$ has a good understanding of how the language works. Hence, a good model has, in theory, a lower perplexity. The exponent $(-1/N)$ in the formula is just a normalizing strategy (geometric average), because adding more characters to a test set would otherwise introduce more uncertainty (i.e. larger test sets would have lower probability). So by introducing the geometric average, we have a metric that is independent of the size of the test set.\n",
    "\n",
    "When calculating the perplexity, it is important to know that taking the product of a bunch of probabilities will most likely lead to a zero value by the computer. To prevent this, make use of a log-transformation:\n",
    "\n",
    "$$\n",
    "\\text{Log-Perplexity}(M) = -\\frac{1}{N} log\\left\\{p(a_1, \\ldots, a_N)\\right\\} = -\\frac{1}{N} \\left\\{log \\ p(a_1) + \\ log \\ p(a_2|a_1) + \\ \\ldots \\ + log \\  p(a_N|a_1, \\ldots, a_{N-1})\\right\\} \n",
    "$$\n",
    "\n",
    "Don't forget to go back to the normal perplexity after this transformation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "709c6d4e"
   },
   "source": [
    "1. Before calculating the perplexity of a test sequence, start with comparing the outputs of 2.2 and 2.3. Do you see any differences in the generated text of the Vanilla RNN model and the GRU model? Rerun your functions a couple of times (because of stochasticity) and use different prompts. Briefly discuss why you would expect (or not expect) certain differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ccf8083"
   },
   "outputs": [],
   "source": [
    "# Your markdown here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5c9a5bf5"
   },
   "source": [
    "2. Calculate the perplexity of each language model by using test_text, an unseen extract of the book. Choose the prompt as the first $m$ letters of the test set, where $m$ is a parameter that you can choose yourself. You should be able to reuse the majority of your previous code in this calculation. Discuss your results at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e0c2a390"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ab728c9"
   },
   "source": [
    "3. As seen in part 2 and 3 of this problem, the text generation is not perfect. Describe some possible model improvements that could make the quality of the generated text better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8df914d2"
   },
   "outputs": [],
   "source": [
    "# Your markdown here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
